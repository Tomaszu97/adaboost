#from timeit import default_timer as timer
# lasttime = timer()
# tottime = timer()
# global lasttime
# lasttime = timer()
# global tottime
# tottime += timer() - lasttime               
#print(f'TIMER={tottime}')





from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn import datasets
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone
from sklearn.metrics import accuracy_score 
from scipy.stats import ttest_rel

clfs ={
    'kNN': KNeighborsClassifier(),
    'GNB': GaussianNB()
}

X, y = datasets.make_classification(
    n_samples=1000,
    n_features=2,
    n_classes=2,
    n_informative=2,
    n_redundant=0,
    n_repeated=0,
    random_state=1905
)


plt.figure()
plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr', alpha=0.3)
plt.grid()
plt.xlabel("$x^1$")
plt.ylabel("$x^2$")
plt.tight_layout()
#plt.show()
plt.savefig('1.png')

folds = 5
results = np.zeros((len(clfs),folds))
skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1410)

for fold, (train, test) in enumerate(skf.split(X, y)):
    for clf_idx, clf_name in enumerate(clfs):
        clf = clone(clfs[clf_name])
        clf.fit(X[train], y[train])
        y_pred = clf.predict(X[test])
        results[clf_idx, fold] = accuracy_score(y[test], y_pred)


for clf_idx, clf_name in enumerate(clfs):
    results_vector = results[clf_idx,]
    print(f'classfier type: {clf_name}')
    print(f'accuracy results : {results_vector}')
    print(f'accuracy (average): {round(np.average(results_vector), 3)}')
    print(f'accuracy (std deviation): {round(np.std(results_vector), 3)}')

alpha = 0.05
test = ttest_rel(results[0], results[1])
T = test.statistic
p = test.pvalue

if p > alpha:
    print('no statistically important differences between classifiers')
elif T > 0:
    print('first classifier is better')
else:
    print('second classifier is better')